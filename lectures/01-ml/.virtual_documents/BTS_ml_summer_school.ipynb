

















# but of course, let's start with the imports
import numpy as np
import pandas as pd
import tensorflow as tf
import wandb
from ast import literal_eval
import scipy

from tensorflow import keras
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Concatenate
























































# we define some basic constants.
config = {
    'image_size': (63,63,3),
    'metadata_size': (14,),
    'dropout_1': 0.3,
    'dropout_2': 0.3,
    'dropout_3': 0.3,
}





# first we prepare the input layers
triplet_input = keras.Input(shape=config["image_size"], name='triplet')
meta_input = keras.Input(shape=config["metadata_size"], name='metadata')





# first conv block
x_conv = Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=(63, 63, 3), name='conv1')(triplet_input)
x_conv = Conv2D(16, (3, 3), activation='relu', padding='same', name='conv2')(x_conv)
x_conv = MaxPooling2D(pool_size=(2, 2), name='pool1')(x_conv)
x_conv = Dropout(config["dropout_1"], name='drop1')(x_conv)

# second conv block
x_conv = Conv2D(32, (3, 3), activation='relu', padding='same', name='conv3')(x_conv)
x_conv = Conv2D(32, (3, 3), activation='relu', padding='same', name='conv4')(x_conv)
x_conv = MaxPooling2D(pool_size=(4, 4), name='pool2')(x_conv)
x_conv = Dropout(config["dropout_2"], name='drop2')(x_conv)

# we flatten the output
x_conv = Flatten()(x_conv)





# Metadata model
x_meta = Dense(16, activation='relu', name='metadata_fc_1')(meta_input)
x_meta = Dense(32, activation='relu', name='metadata_fc_2')(x_meta)





# Merged model
x = Concatenate(axis=1)([x_conv, x_meta])
x = Dense(16, activation='relu', name='comb_fc_2')(x)
x = Dropout(config["dropout_3"])(x)

# Output (binary classification, a single sigmoid neuron giving us a score between 0 (not BTS) and 1 (BTS!!!))
output = Dense(1, activation='sigmoid', name='fc_out')(x)





model = keras.Model(inputs=[triplet_input, meta_input], outputs=output, name="mi_cnn")











fake_triplet = np.zeros((1, 63, 63, 3))
fake_meta = np.zeros((1, 14))

model.predict([fake_triplet, fake_meta])





def BTSModel(config):
    #image size and metadata size might be strings, so we convert them to tuples with a literal eval
    if type(config["image_size"]) == str:
        image_size = literal_eval(config["image_size"])
    else:
        image_size = config["image_size"]
    if type(config["metadata_size"]) == str:
        metadata_size = literal_eval(config["metadata_size"])
    else:
        metadata_size = config["metadata_size"]

    # first we prepare the input layers
    triplet_input = keras.Input(shape=image_size, name='triplet')
    meta_input = keras.Input(shape=metadata_size, name='metadata')

    # first conv block
    x_conv = Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=(63, 63, 3), name='conv1')(triplet_input)
    x_conv = Conv2D(16, (3, 3), activation='relu', padding='same', name='conv2')(x_conv)
    x_conv = MaxPooling2D(pool_size=(2, 2), name='pool1')(x_conv)
    x_conv = Dropout(config["dropout_1"], name='drop1')(x_conv)

    # second conv block
    x_conv = Conv2D(32, (3, 3), activation='relu', padding='same', name='conv3')(x_conv)
    x_conv = Conv2D(32, (3, 3), activation='relu', padding='same', name='conv4')(x_conv)
    x_conv = MaxPooling2D(pool_size=(4, 4), name='pool2')(x_conv)
    x_conv = Dropout(config["dropout_2"], name='drop2')(x_conv)

    # we flatten the output
    x_conv = Flatten()(x_conv)

    # Metadata model
    x_meta = Dense(16, activation='relu', name='metadata_fc_1')(meta_input)
    x_meta = Dense(32, activation='relu', name='metadata_fc_2')(x_meta)

    # Merged model
    x = Concatenate(axis=1)([x_conv, x_meta])
    x = Dense(16, activation='relu', name='comb_fc_2')(x)
    x = Dropout(config["dropout_3"])(x)

    # Output (binary classification, a single sigmoid neuron giving us a score between 0 (not BTS) and 1 (BTS!!!))
    output = Dense(1, activation='sigmoid', name='fc_out')(x)

    model = keras.Model(inputs=[triplet_input, meta_input], outputs=output, name="mi_cnn")

    return model








# we add to our config some of the constants used in the training process
config.update({
    'batch_size': 8,
    'epochs': 10,
    'learning_rate': 0.001,
    'loss': 'binary_crossentropy',
    'optimizer': 'adam',
    'random_seed': 42,
    'train_data_version': "ZTFSS",
    'early_stopping_patience': 10,
    'LR_plateau_patience': 20,
    'reduce_LR_factor': 0.5,
    'reduce_LR_minLR': 1e-6, 
    'beta_1': 0.9,
    'beta_2': 0.999,
    'metadata_cols': [
      "sgscore1",
      "distpsnr1",
      "sgscore2",
      "distpsnr2",
      "fwhm",
      "magpsf",
      "sigmapsf",
      "ra",
      "dec",
      "diffmaglim",
      "ndethist",
      "nmtchps",
      "age",
      "peakmag_so_far"
    ],
})


tf.keras.backend.clear_session()
tf.keras.utils.set_random_seed(config["random_seed"])





cand = pd.read_csv(f'data/train_cand_{config["train_data_version"]}.csv')
triplets = np.load(f'data/train_triplets_{config["train_data_version"]}.npy', mmap_mode='r+')

# split the data into training and validation sets
val_indexes = np.random.choice(cand.index, size=int(len(cand) * 0.25), replace=False)
train_indexes = np.array(list(set(cand.index) - set(val_indexes)))

val_cand = cand.loc[val_indexes]
val_triplets = triplets[np.isin(cand.index, val_indexes)]

triplets = triplets[np.isin(cand.index, train_indexes)]
cand = cand.loc[train_indexes]

# print the shape of everything
print(f"Training set: {cand.shape[0]} candidates, {triplets.shape[0]} triplets")
print(f"Validation set: {val_cand.shape[0]} candidates, {val_triplets.shape[0]} triplets")

gen_cols = np.append(config['metadata_cols'], ['label'])

x_train, y_train = triplets, cand['label']
x_val, y_val = val_triplets, val_cand['label']

# train_df is a combination of the desired metadata cols and y_train (labels)
# we provide the model a custom generator function to separate these as necessary
train_df = cand[gen_cols]
val_df = val_cand[gen_cols]






train_datagen = tf.keras.preprocessing.image.ImageDataGenerator()
val_datagen = tf.keras.preprocessing.image.ImageDataGenerator()

t_generator = train_datagen.flow(x_train, train_df, batch_size=config["batch_size"], seed=config["random_seed"], shuffle=False)
v_generator = val_datagen.flow(x_val, val_df, batch_size=config["batch_size"], seed=config["random_seed"], shuffle=False)

def multiinput_train_generator():
    while True:
        # get the data from the generator
        # data is [[img], [metadata and labels]]
        # yields batch_size number of entries
        data = t_generator.next()

        imgs = data[0]
        cols = data[1][:,:-1]
        targets = data[1][:,-1:]

        yield [imgs, cols], targets

def multiinput_val_generator():
    while True:
        data = v_generator.next()

        imgs = data[0]
        cols = data[1][:,:-1]
        targets = data[1][:,-1:]

        yield [imgs, cols], targets

training_generator = multiinput_train_generator()
validation_generator = multiinput_val_generator()

# weight data on number of ALERTS per class
num_training_examples_per_class = np.array([np.sum(cand['label'] == 0), np.sum(cand['label'] == 1)])

# fewer examples -> larger weight
weights = (1 / num_training_examples_per_class) / np.linalg.norm((1 / num_training_examples_per_class))
normalized_weight = weights / np.max(weights)

class_weight = {i: w for i, w in enumerate(normalized_weight)}





# we set some rules to stop the training process early if the validation loss does not improve
# halt training if no improvement in validation loss over patience epochs
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    verbose=1, 
    patience=config['early_stopping_patience']
)

# reduce learning rate if no improvement in validation loss over patience epochs
LR_plateau = tf.keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss", 
    patience=config['LR_plateau_patience'],
    factor=config['reduce_LR_factor'],
    min_lr=config['reduce_LR_minLR'],
    verbose=0
)





wandb.init(project="BTSbot")
# Send parameters of this run to WandB
for param in list(config):
    wandb.config[param] = config[param]

run_name = wandb.run.name
WandBLogger = wandb.keras.WandbMetricsLogger(log_freq=5)





optimizer = tf.keras.optimizers.Adam(
    learning_rate=config['learning_rate'], 
    beta_1=config['beta_1'],
    beta_2=config['beta_2']
)
model.compile(optimizer=optimizer, loss=config['loss'], metrics=['accuracy'])

#PS: If you are using tensorflow v2.11+ on M1/M2/M3 macs, you might benefit from using
# the legacy version of the optimizer, which is: tf.keras.optimizers.legacy.Adam, instead of tf.keras.optimizers.Adam





h = model.fit(
    training_generator,
    steps_per_epoch=0.8*len(x_train) // config["batch_size"],
    validation_data=validation_generator,
    validation_steps=(0.8*len(x_val)) // config["batch_size"],
    epochs=config["epochs"],
    verbose=1, callbacks=[early_stopping, LR_plateau, WandBLogger]
)





raw_preds = model.predict([val_triplets, val_cand.loc[:,config["metadata_cols"]]], batch_size=config['batch_size'], verbose=1)
preds = np.rint(np.transpose(raw_preds))[0].astype(int)
labels = val_cand["label"].to_numpy(dtype=int)

results = preds == labels
print(f"Overall validation accuracy {100*np.sum(results) / len(results):.2f}%")














def train(config):
    WandBLogger = wandb.keras.WandbMetricsLogger(log_freq=5)
    loss = config['loss']
    metadata_cols = [
        "sgscore1",
        "distpsnr1",
        "sgscore2",
        "distpsnr2",
        "fwhm",
        "magpsf",
        "sigmapsf",
        "ra",
        "dec",
        "diffmaglim",
        "ndethist",
        "nmtchps",
        "age",
        "peakmag_so_far"
    ]
    tf.keras.backend.clear_session()
    tf.keras.utils.set_random_seed(config["random_seed"])
    
    cand = pd.read_csv(f'data/train_cand_{config["train_data_version"]}.csv')
    triplets = np.load(f'data/train_triplets_{config["train_data_version"]}.npy', mmap_mode='r+')
    
    # set the random seed, so we can reproduce the results
    np.random.seed(config.get("random_seed", 20240729))

    # split the data into training and validation sets
    val_indexes = np.random.choice(cand.index, size=int(len(cand) * 0.2), replace=False)
    train_indexes = np.array(list(set(cand.index) - set(val_indexes)))

    val_cand = cand.loc[val_indexes]
    val_triplets = triplets[np.isin(cand.index, val_indexes)]

    triplets = triplets[np.isin(cand.index, train_indexes)]
    cand = cand.loc[train_indexes]

    gen_cols = np.append(metadata_cols, ['label'])

    x_train, y_train = triplets, cand['label']
    x_val, y_val = val_triplets, val_cand['label']

    train_df = cand[gen_cols]
    val_df = val_cand[gen_cols]


    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        verbose=1, 
        patience=config['early_stopping_patience']
    )

    LR_plateau = tf.keras.callbacks.ReduceLROnPlateau(
        monitor="val_loss", 
        patience=config['LR_plateau_patience'],
        factor=config['reduce_LR_factor'],
        min_lr=config['reduce_LR_minLR'],
        verbose=0
    )

    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator()
    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator()

    t_generator = train_datagen.flow(x_train, train_df, batch_size=config["batch_size"], seed=config["random_seed"], shuffle=False)
    v_generator = val_datagen.flow(x_val, val_df, batch_size=config["batch_size"], seed=config["random_seed"], shuffle=False)

    def multiinput_train_generator():
        while True:
            # get the data from the generator
            # data is [[img], [metadata and labels]]
            # yields batch_size number of entries
            data = t_generator.next()

            imgs = data[0]
            cols = data[1][:,:-1]
            targets = data[1][:,-1:]

            yield [imgs, cols], targets

    def multiinput_val_generator():
        while True:
            data = v_generator.next()

            imgs = data[0]
            cols = data[1][:,:-1]
            targets = data[1][:,-1:]

            yield [imgs, cols], targets

    training_generator = multiinput_train_generator()
    validation_generator = multiinput_val_generator()

    model = BTSModel(config)
    # you might want to replace tf.keras.optimizers.Adam with tf.keras.optimizers.legacy.Adam if you are using tensorflow v2.11+ on M1/M2/M3 macs
    optimizer = tf.keras.optimizers.legacy.Adam(
        learning_rate=config['learning_rate'], 
        beta_1=config['beta_1'],
        beta_2=config['beta_2']
    )

    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])
    
    model.fit(
        training_generator,
        steps_per_epoch=0.8*len(x_train) // config["batch_size"],
        validation_data=validation_generator,
        validation_steps=(0.8*len(x_val)) // config["batch_size"],
        #class_weight=class_weight,
        epochs=config["epochs"],
        verbose=1, callbacks=[early_stopping, LR_plateau, WandBLogger]
    )

    raw_preds = model.predict([triplets, cand.loc[:,metadata_cols]], batch_size=config['batch_size'], verbose=1)
    preds = np.rint(np.transpose(raw_preds))[0].astype(int)
    labels = cand["label"].to_numpy(dtype=int)

    results = preds == labels
    print(f"Overall validation accuracy {100*np.sum(results) / len(results):.2f}%")
    


# quick wrapper around the train function so it can be called nicely by a wandb sweep agent
def sweep_train(config=None):
    with wandb.init(config=config) as run:
        train(run.config)








# create a sweep on WandB by going to the project page and clicking "Sweep"
sweep_id = None #replace with your sweep id here
if not sweep_id:
    raise Exception("Please provide a sweep id")
wandb.agent(sweep_id=sweep_id, function=sweep_train, count=10, project="BTSbot")






